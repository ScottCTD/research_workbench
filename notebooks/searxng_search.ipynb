{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18ef2fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query', 'number_of_results', 'results', 'answers', 'corrections', 'infoboxes', 'suggestions', 'unresponsive_engines'])\n",
      "Top result: [\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://futureoflife.org/ai-safety-index-winter-2025/\",\n",
      "    \"title\": \"AI Safety Index Winter 2025 - Future of Life Institute\",\n",
      "    \"content\": \"TrustLLM was developed by 45 research ... U.S. ... TrustLLM evaluates how reliably AI systems uphold truthfulness, privacy, and ethical reasoning beyond standard capability metrics. Strong performance indicates that companies have invested in aligning their models to be harmless and helpful, and not to cause unintended harm. ... Measures AI safety behaviors including resistance to misuse, appropriate refusals, calibration accuracy, honesty under pressure, and ethical restraint in scenarios...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"futureoflife.org\",\n",
      "      \"/ai-safety-index-winter-2025/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\",\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      1,\n",
      "      6\n",
      "    ],\n",
      "    \"score\": 2.3333333333333335,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.anthropic.com/research\",\n",
      "    \"title\": \"Research - Anthropic\",\n",
      "    \"content\": \"Our research teams investigate the safety, inner workings, and societal impacts of AI models \\u2013 so that artificial intelligence has a positive impact as it\\u00a0...\",\n",
      "    \"publishedDate\": \"2025-12-17T22:36:30.408258\",\n",
      "    \"engine\": \"startpage\",\n",
      "    \"template\": \"default.html\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.anthropic.com\",\n",
      "      \"/research\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"thumbnail\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      2\n",
      "    ],\n",
      "    \"score\": 0.5,\n",
      "    \"category\": \"general\",\n",
      "    \"pubdate\": \"2025-12-17 22:36:30\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.aisi.gov.uk/frontier-ai-trends-report\",\n",
      "    \"title\": \"Frontier AI Trends Report by The AI Security Institute (AISI)\",\n",
      "    \"content\": \"In this section, we describe how AI capability improvements enable new possibilities in three domains critical to security and innovation: chemistry & biology,\\u00a0...\",\n",
      "    \"publishedDate\": \"2025-12-19T22:36:30.408427\",\n",
      "    \"engine\": \"startpage\",\n",
      "    \"template\": \"default.html\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.aisi.gov.uk\",\n",
      "      \"/frontier-ai-trends-report\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"thumbnail\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      5\n",
      "    ],\n",
      "    \"score\": 0.2,\n",
      "    \"category\": \"general\",\n",
      "    \"pubdate\": \"2025-12-19 22:36:30\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://deepmind.google/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/\",\n",
      "    \"title\": \"Gemma Scope 2: Helping the AI Safety Community Deepen ...\",\n",
      "    \"content\": \"In turn, this enables the richer study of jailbreaks or other AI behaviours relevant to safety, like discrepancies between a model's communicated reasoning and\\u00a0...\",\n",
      "    \"publishedDate\": \"2025-12-19T22:36:30.408541\",\n",
      "    \"engine\": \"startpage\",\n",
      "    \"template\": \"default.html\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"deepmind.google\",\n",
      "      \"/blog/gemma-scope-2-helping-the-ai-safety-community-deepen-understanding-of-complex-language-model-behavior/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"thumbnail\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      7\n",
      "    ],\n",
      "    \"score\": 0.14285714285714285,\n",
      "    \"category\": \"general\",\n",
      "    \"pubdate\": \"2025-12-19 22:36:30\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://arxiv.org/abs/2507.11473\",\n",
      "    \"title\": \"[2507.11473] Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety\",\n",
      "    \"content\": \"View a PDF of the paper titled Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety, by Tomek Korbak and 40 other authors View PDF HTML (experimental)\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"arxiv.org\",\n",
      "      \"/abs/2507.11473\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      8\n",
      "    ],\n",
      "    \"score\": 0.125,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://www.independent.co.uk/tech/most-harmful-ai-app-chatgpt-gemini-alibaba-b2880884.html\",\n",
      "    \"title\": \"'Most harmful' AI revealed in new safety study | The Independent\",\n",
      "    \"content\": \"Dec 9, 2025 ... US companies scored the best marks in the AI Safety Index, with Anthropic ahead of ChatGPT creator OpenAI and Google DeepMind. Chinese companies recorded the\\u00a0...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"engine\": \"startpage\",\n",
      "    \"template\": \"default.html\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.independent.co.uk\",\n",
      "      \"/tech/most-harmful-ai-app-chatgpt-gemini-alibaba-b2880884.html\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"thumbnail\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      8\n",
      "    ],\n",
      "    \"score\": 0.125,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"url\": \"https://internationalaisafetyreport.org/publication/second-key-update-technical-safeguards-and-risk-management\",\n",
      "    \"title\": \"Second Key Update: Technical Safeguards and Risk Management\",\n",
      "    \"content\": \"Nov 25, 2025 ... Since the publication of the 2025 International AI Safety Report, the number of companies publishing Frontier AI Safety Frameworks has more than doubled, and\\u00a0...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"engine\": \"startpage\",\n",
      "    \"template\": \"default.html\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"internationalaisafetyreport.org\",\n",
      "      \"/publication/second-key-update-technical-safeguards-and-risk-management\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"thumbnail\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      9\n",
      "    ],\n",
      "    \"score\": 0.1111111111111111,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.nbcnews.com/tech/tech-news/top-ai-companies-safety-practices-fall-short-says-new-report-rcna246143\",\n",
      "    \"title\": \"Leading AI companies' safety practices are falling short, new report says\",\n",
      "    \"content\": \"Dec 3, 2025 ... The Winter 2025 AI Safety Index, which examines the safety protocols of eight leading AI companies, found that their approaches \\u201clack the concrete safeguards,\\u00a0...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/hg7HKFsZhTaeOUWvG4dx3CCMgvBikEiIsS0d_9zhwFA/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9tZWRp/YS1jbGRucnkucy1u/YmNuZXdzLmNvbS9p/bWFnZS91cGxvYWQv/dF9maXQtMTUwMHcs/Zl9hdXRvLHFfYXV0/bzpiZXN0L3JvY2tj/bXMvMjAyNS0xMC8y/NTEwMDEtb3Blbi1h/aS12bC0xMTlwLTEx/NWE5Yy5qcGc\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.nbcnews.com\",\n",
      "      \"/tech/tech-news/top-ai-companies-safety-practices-fall-short-says-new-report-rcna246143\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\",\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      6,\n",
      "      1\n",
      "    ],\n",
      "    \"score\": 2.3333333333333335,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.reuters.com/business/ai-companies-safety-practices-fail-meet-global-standards-study-shows-2025-12-03/\",\n",
      "    \"title\": \"AI companies' safety practices fail to meet global standards, study shows | Reuters\",\n",
      "    \"content\": \"REUTERS/Dado Ruvic/Illustration Purchase Licensing Rights, opens new tab \\u00b7 Dec 3 (Reuters) - The safety practices of major artificial intelligence companies, such as Anthropic, OpenAI, xAI and Meta, are \\\"far short of emerging global standards,\\\" ...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/MkMQ5uFXqlIFYvw8etUfFO4wb1aFqE0JcoCQwgUSuCA/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmV1dGVycy5jb20v/cmVzaXplci92Mi82/QzJQWE9XNjZWSzVS/SU00QzczV1g2RkVC/VS5qcGc_YXV0aD1m/OTg2ODBiNTI3ZGVi/ZDgxODE2OGJiY2Ey/ZjcwYjQ2Zjg0MDZj/MTdkNjI5YWI4YjI5/MWQ5M2Q4YzU0ZjQx/MTcxJmhlaWdodD0x/MDA1JndpZHRoPTE5/MjAmcXVhbGl0eT04/MCZzbWFydD10cnVl\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.reuters.com\",\n",
      "      \"/business/ai-companies-safety-practices-fail-meet-global-standards-study-shows-2025-12-03/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\",\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      3,\n",
      "      4\n",
      "    ],\n",
      "    \"score\": 1.1666666666666665,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://mashable.com/article/ai-safety-report-2025-chatgpt-gemini-claude\",\n",
      "    \"title\": \"AI safety experts say most models are failing | Mashable\",\n",
      "    \"content\": \"Dec 3, 2025 ... The winter 2025 AI Safety Index, published by tech research non-profit Future of Life Institute (FLI), surveyed eight AI providers \\u2014 OpenAI, DeepSeek, Google,\\u00a0...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/O4TFWKm14IC0nj44TmVAt0LPHomz2Jd6CLMISk6zU7g/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4u/ZXguY28vdHJhbnNm/b3JtYXRpb25zL3By/b2R1Y3Rpb24vODI5/N2M1ZjMtNjQwNS00/Y2U0LWI3OTYtYjA1/NzZhNWU4Yjk4L3Ro/dW1ibmFpbC03MjAu/d2VicA\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"mashable.com\",\n",
      "      \"/article/ai-safety-report-2025-chatgpt-gemini-claude\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\",\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      14,\n",
      "      3\n",
      "    ],\n",
      "    \"score\": 0.8095238095238095,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://futureoflife.org/ai-safety-index-summer-2025/\",\n",
      "    \"title\": \"2025 AI Safety Index - Future of Life Institute\",\n",
      "    \"content\": \"The Summer 2025 version of the Index evaluates seven leading AI companies on an improved set of 33 indicators of responsible AI development and deployment practices, spanning six critical domains.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/vyJu3urGGYv1a0VmoZXA12-t1GptspitByffg6Glxhs/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9mdXR1/cmVvZmxpZmUub3Jn/L3dwLWNvbnRlbnQv/dXBsb2Fkcy8yMDI1/LzA3L0ZMSS1BSS1T/YWZldHktSW5kZXgt/UmVwb3J0LVN1bW1l/ci0yMDI1LU9wZW5H/cmFwaC1zY2FsZWQu/anBn\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"futureoflife.org\",\n",
      "      \"/ai-safety-index-summer-2025/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      2\n",
      "    ],\n",
      "    \"score\": 0.5,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://en.wikipedia.org/wiki/AI_safety\",\n",
      "    \"title\": \"AI safety - Wikipedia\",\n",
      "    \"content\": \"In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety. In 2023, Rishi Sunak said he wants the United Kingdom to be the \\\"geographical home of global AI safety regulation\\\" and to host the first global summit ...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/QcoqANHeeUmLrL_Xl0vzotIJXevpHRbfexStMhsjnbc/rs:fit:500:0:0:0/g:ce/aHR0cHM6Ly91cGxv/YWQud2lraW1lZGlh/Lm9yZy93aWtpcGVk/aWEvY29tbW9ucy8z/LzM2L1ZpY2VfUHJl/c2lkZW50X0hhcnJp/c19hdF90aGVfZ3Jv/dXBfcGhvdG9fb2Zf/dGhlXzIwMjNfQUlf/U2FmZXR5X1N1bW1p/dC5qcGc\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"en.wikipedia.org\",\n",
      "      \"/wiki/AI_safety\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      5,\n",
      "      21\n",
      "    ],\n",
      "    \"score\": 0.49523809523809526,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.latimes.com/entertainment-arts/business/story/2025-12-05/ai-artificial-intelligence-company-scorecard-ranks-safety-humanity\",\n",
      "    \"title\": \"A safety report card ranks AI company efforts to protect humanity - Los Angeles Times\",\n",
      "    \"content\": \"... \\u201cThey are the only industry ... Max Tegmark in an interview. ... OpenAI is the latest tech company to face a lawsuit alleging chatbots are providing teens with self-harm content....\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/BqGrYGkbTC5Ah5FX6uyU22tXOXBnoW0ZVo6iVX3I9CM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jYS10/aW1lcy5icmlnaHRz/cG90Y2RuLmNvbS9k/aW1zNC9kZWZhdWx0/LzFlN2JjYWIvMjE0/NzQ4MzY0Ny9zdHJp/cC9mYWxzZS9jcm9w/LzQ1NjR4MzA0Mysw/KzAvcmVzaXplLzE0/ODZ4OTkxIS9xdWFs/aXR5Lzc1Lz91cmw9/aHR0cHMlM0ElMkYl/MkZjYWxpZm9ybmlh/LXRpbWVzLWJyaWdo/dHNwb3QuczMuYW1h/em9uYXdzLmNvbSUy/RjcxJTJGZmYlMkZl/MzJjMjc3ZTQ4NTRh/MjZkMmQ3NzQyNGFl/YmU0JTJGMjIzNTA1/NzUwNS5qcGc\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.latimes.com\",\n",
      "      \"/entertainment-arts/business/story/2025-12-05/ai-artificial-intelligence-company-scorecard-ranks-safety-humanity\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"startpage\",\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      7,\n",
      "      10\n",
      "    ],\n",
      "    \"score\": 0.4857142857142857,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.theguardian.com/technology/2025/nov/30/ai-poetry-safety-features-jailbreak\",\n",
      "    \"title\": \"AI\\u2019s safety features can be circumvented with poetry, research finds | Artificial intelligence (AI) | The Guardian\",\n",
      "    \"content\": \"Researchers tested two Meta AI models and both responded to 70% of the poetic prompts with harmful responses, according to the study. Meta declined to comment on the findings. None of the other companies involved in the research responded to ...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/w45H3bvkyabhOtLK9ozZISRcplhCNQfw8a6jCguAa1Y/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pLmd1/aW0uY28udWsvaW1n/L21lZGlhL2QxZTky/ZjhmOWNkY2RmYWVj/ZWRiYjFhMDIyYWUz/MDAyYTVmNmI5YzMv/MF81OTFfMjgzMl8y/MjY0L21hc3Rlci8y/ODMyLmpwZz93aWR0/aD0xMjAwJmhlaWdo/dD02MzAmcXVhbGl0/eT04NSZhdXRvPWZv/cm1hdCZmaXQ9Y3Jv/cCZwcmVjcm9wPTQw/OjIxLG9mZnNldC14/NTAsb2Zmc2V0LXkw/Jm92ZXJsYXktYWxp/Z249Ym90dG9tJTJD/bGVmdCZvdmVybGF5/LXdpZHRoPTEwMHAm/b3ZlcmxheS1iYXNl/NjQ9TDJsdFp5OXpk/R0YwYVdNdmIzWmxj/bXhoZVhNdmRHY3Ra/R1ZtWVhWc2RDNXdi/bWMmZW5hYmxlPXVw/c2NhbGUmcz0yZWY2/NDFjZjE0NDE3NmRk/MDA1NTkxMDExNzg2/YWU2Mg\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.theguardian.com\",\n",
      "      \"/technology/2025/nov/30/ai-poetry-safety-features-jailbreak\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      4\n",
      "    ],\n",
      "    \"score\": 0.25,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://qz.com/ai-companies-fail-existential-safety-test-as-capabilities-race-ahead\",\n",
      "    \"title\": \"Major AI companies fail safety test for superintelligence\",\n",
      "    \"content\": \"... The Future of Life Institute's winter 2025 AI Safety Index evaluated eight major AI companies across six dimensions, including risk assessment, current harms, and existential safety.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/pufG-VHCHCRnHB03jNCj2ybeo36lDK3yqzWaEqlSDww/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9xei5j/b20vY2RuLWNnaS9p/bWFnZS93aWR0aD0z/MDAsaGVpZ2h0PTMw/MCxmaXQ9Y292ZXIs/cXVhbGl0eT04NSxm/b3JtYXQ9YXV0by9o/dHRwczovL2Fzc2V0/cy5xei5jb20vbWVk/aWEvR2V0dHlJbWFn/ZXMtMjI0NzE1NTE4/NC5qcGc\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"qz.com\",\n",
      "      \"/ai-companies-fail-existential-safety-test-as-capabilities-race-ahead\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      9\n",
      "    ],\n",
      "    \"score\": 0.1111111111111111,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.the-independent.com/tech/most-harmful-ai-app-chatgpt-gemini-alibaba-b2880884.html\",\n",
      "    \"title\": \"\\u2018Most harmful\\u2019 AI revealed in new safety study | The Independent\",\n",
      "    \"content\": \"No company scored above a D when evaluated for existential risk, while Alibaba Cloud, DeepSeek, Meta, xAI and Z.ai all scored an F. \\u201cExistential safety remains the sector\\u2019s core structural failure, making the widening gap between accelerating AGI/ superintelligence ambitions and the absence of credible control plans increasingly alarming,\\u201d the study noted.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/mHO1p06-jl1-AMSDHDqoRl_mL7_aDDDxntk5BZNfrLk/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zdGF0/aWMudGhlLWluZGVw/ZW5kZW50LmNvbS8y/MDI1LzEyLzA5LzEy/LzUwL2FpLWFwcHMt/bW9zdC1kYW5nZXJv/dXMuanBlZz93aWR0/aD0xMjAw\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.the-independent.com\",\n",
      "      \"/tech/most-harmful-ai-app-chatgpt-gemini-alibaba-b2880884.html\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      10\n",
      "    ],\n",
      "    \"score\": 0.1,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.industry.gov.au/science-technology-and-innovation/technology/artificial-intelligence/ai-safety-science\",\n",
      "    \"title\": \"AI safety science | Department of Industry Science and Resources\",\n",
      "    \"content\": \"Australia attended the third directors-level meeting of the International Network of AI Safety Institutes in Vancouver this July. ... We've commissioned research by the Gradient Institute to strengthen AI safety science and support responsible deployment of AI agents.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/J0Jtay4TbomN26rTjbXGEH368_j8YG9OR2p2FaHgXUY/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/aW5kdXN0cnkuZ292/LmF1L3NpdGVzL2Rl/ZmF1bHQvZmlsZXMv/MjAyNS0wNy9haS1z/YWZldHktc2NpZW5j/ZS1ncmFkaWVudC1p/bnN0aXR1dGUtZGVm/YXVsdC1pbWFnZS5q/cGc\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.industry.gov.au\",\n",
      "      \"/science-technology-and-innovation/technology/artificial-intelligence/ai-safety-science\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      11\n",
      "    ],\n",
      "    \"score\": 0.09090909090909091,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.eweek.com/news/openai-laid-out-2026-roadmap-neuron/\",\n",
      "    \"title\": \"OpenAI Just Laid Out Its 2026 Roadmap\",\n",
      "    \"content\": \"And because \\u201ctrust me, bro\\u201d is not a solid safety strategy, OpenAI also published a chain-of-thought monitorability to measure how detectable bad intent is inside a model\\u2019s reasoning. Very, Very cool project we\\u2019ll want to dig deeper into w/ more time. Zooming out: The US government is trying to speedrun science, too. The DOE announced 24 new Genesis Mission partners, from Nvidia and Microsoft to OpenAI, Anthropic, AWS, CoreWeave, AMD, Intel, and more, all aimed at wiring AI models, cloud, and national labs into a single discovery machine.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/-KIUjNv5ZVAUkQvmYtLEMFrEzGBu9C4krYHtwpSg7mA/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9hc3Nl/dHMuZXdlZWsuY29t/L3VwbG9hZHMvMjAy/NS8xMi9zb2x1dGlv/bi1jb25jZXB0LXdp/dGgtaHVtYW4tZmVl/dC1pbi1mcm9udC1v/Zi1oYW5kdy0yMDI1/LTEwLTE1LTAyLTM3/LTMwLXV0Yy0xLmpw/Zw\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.eweek.com\",\n",
      "      \"/news/openai-laid-out-2026-roadmap-neuron/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      12\n",
      "    ],\n",
      "    \"score\": 0.08333333333333333,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://thebulletin.org/premium/2025-12/stopping-the-clock-on-catastrophic-ai-risk/\",\n",
      "    \"title\": \"Stopping the Clock on catastrophic AI risk - Bulletin of the Atomic Scientists\",\n",
      "    \"content\": \"The 2025 International AI Safety Report notes significant increases in capabilities relevant to cyber- and bio-risk relative to the interim report released eight months previously; and notable updates in the risk levels of models have been issued ...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/-pPseBDLLAEclbI9TaY0BTjC1GTLzKPZjQkr6GF25T0/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly90aGVi/dWxsZXRpbi5vcmcv/d3AtY29udGVudC91/cGxvYWRzLzIwMTYv/MDQvMV9UaGVfQm9t/Yi5qcGcub3B0aW1h/bC5qcGc\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"thebulletin.org\",\n",
      "      \"/premium/2025-12/stopping-the-clock-on-catastrophic-ai-risk/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      13\n",
      "    ],\n",
      "    \"score\": 0.07692307692307693,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://forum.effectivealtruism.org/posts/AHzW8H3k575Scpm5D/spar-spring-2026-130-research-projects-now-accepting\",\n",
      "    \"title\": \"SPAR Spring 2026: 130+ research projects now accepting applications \\u2014 EA Forum\",\n",
      "    \"content\": \"TL;DR: SPAR is accepting mentee applications for Spring 2026, our largest round yet with 130+ projects across AI safety, governance, security, and (new this round) biosecurity. The program runs from February 16 to May 16.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/hKgydaPZEODCrVQAa4Rwm0Di-0r-GO0QMIMPiL2uYtw/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9yZXMu/Y2xvdWRpbmFyeS5j/b20vY2VhL2ltYWdl/L3VwbG9hZC9xX2F1/dG8sZl9hdXRvLGNf/bGZpbGwsd18xMjAw/LGFyXzEuOSxnX2F1/dG8vU29jaWFsUHJl/dmlldy9vdm51bmp6/MzNpbzU1cWJ4Zm5o/eA\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"forum.effectivealtruism.org\",\n",
      "      \"/posts/AHzW8H3k575Scpm5D/spar-spring-2026-130-research-projects-now-accepting\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      16\n",
      "    ],\n",
      "    \"score\": 0.0625,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.newstheai.com/news/articleView.html?idxno=10220\",\n",
      "    \"title\": \"Why Companies Are Investing in AI Safety Solutions\",\n",
      "    \"content\": \"According to market research firm MarketsandMarkets, the global industrial safety market is expected to grow from USD 7.7 billion in 2025 to USD 10.6 billion by 2030, at a compound annual growth rate (CAGR) of 6.5%. Another research firm, SNS ...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/kGo7Ntyo1q5ZAT-MaWoV693lk4OzI0ANupJZfBYBO58/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9jZG4u/bmV3c3RoZWFpLmNv/bS9uZXdzL3RodW1i/bmFpbC8yMDI1MTIv/MTAyMjBfMTUzMjZf/NDM0MF92MTUwLmpw/Zw\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.newstheai.com\",\n",
      "      \"/news/articleView.html\",\n",
      "      \"\",\n",
      "      \"idxno=10220\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      17\n",
      "    ],\n",
      "    \"score\": 0.058823529411764705,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.thecooldown.com/green-business/ai-safety-companies-index-expansion/\",\n",
      "    \"title\": \"New report finds dangerously overlooked flaw in leading AI companies' systems: 'Existential risk of the superintelligent systems'\",\n",
      "    \"content\": \"The Winter 2025 AI Safety Index revealed major AI companies aren't taking responsibility for the potentially harmful technology they're creating.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/kZw3gUm9xYM1zWvMJMeDPndNBeMI8Z2-IDEy4bEwqBM/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/dGhlY29vbGRvd24u/Y29tL3dwLWNvbnRl/bnQvdXBsb2Fkcy8y/MDI1LzEyL2lTdG9j/ay0yMjMyNjIzNjc5/LmpwZw\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.thecooldown.com\",\n",
      "      \"/green-business/ai-safety-companies-index-expansion/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      18\n",
      "    ],\n",
      "    \"score\": 0.05555555555555555,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://techxplore.com/news/2025-12-safety-card-ai-company-efforts.html\",\n",
      "    \"title\": \"A safety report card ranks AI company efforts to protect humanity\",\n",
      "    \"content\": \"Are artificial intelligence companies keeping humanity safe from AI's potential harms? Don't bet on it, a new report card says.\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/VmN-8rTbq6CsJBnoL9vHVf3PPn6F8Uizu2pGN36AEX0/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9zY3gy/LmItY2RuLm5ldC9n/ZngvbmV3cy9oaXJl/cy8yMDE4LzEtY3li/ZXJhdHRhY2suanBn\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"techxplore.com\",\n",
      "      \"/news/2025-12-safety-card-ai-company-efforts.html\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      19\n",
      "    ],\n",
      "    \"score\": 0.05263157894736842,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"default.html\",\n",
      "    \"url\": \"https://www.red94.net/news/51905-artificial-intelligence-companies-safety-practices-are-falling-short-of-global-s/\",\n",
      "    \"title\": \"Artificial intelligence companies reveal shocking truth about safety planning, and it's worse than anyone feared\",\n",
      "    \"content\": \"Major artificial intelligence companies ... reveals. The 2025 Winter AI Safety Index report from the Future of Life Institute shows none of the eight evaluated companies have credible plans to control superintelligent systems...\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/EFeQ0u86O-2UGz3w6KX3eYKqxRm6lSIQs5rnKxhNasg/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly93d3cu/cmVkOTQubmV0L25l/d3Mvd3AtY29udGVu/dC91cGxvYWRzLzIw/MjUvMTIvNjI0MTEt/YXJ0aWZpY2lhbC1p/bnRlbGxpZ2VuY2Ut/Y29tcGFuaWVzLXNh/ZmV0eS1wcmFjdGlj/ZXMtYXJlLWZhbGxp/bmctc2hvcnQtb2Yt/Z2xvYmFsLXMuanBn/LnBuZw\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.red94.net\",\n",
      "      \"/news/51905-artificial-intelligence-companies-safety-practices-are-falling-short-of-global-s/\",\n",
      "      \"\",\n",
      "      \"\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      20\n",
      "    ],\n",
      "    \"score\": 0.05,\n",
      "    \"category\": \"general\"\n",
      "  },\n",
      "  {\n",
      "    \"template\": \"videos.html\",\n",
      "    \"url\": \"https://www.youtube.com/watch?v=MoIpDVF79x8\",\n",
      "    \"title\": \"AI Safety Expert: Will AI Destroy Humanity? - YouTube\",\n",
      "    \"content\": \"AboutPressCopyrightContact usCreatorsAdvertiseDevelopersTermsPrivacyPolicy & SafetyHow YouTube worksTest new featuresNFL Sunday Ticket Published 4 weeks ago\",\n",
      "    \"publishedDate\": null,\n",
      "    \"thumbnail\": \"https://imgs.search.brave.com/lLUhXGCBqnJl7CTbiOnxEkYs1l-g6TtVMLdyNcSPGZI/rs:fit:200:200:1:0/g:ce/aHR0cHM6Ly9pLnl0/aW1nLmNvbS92aS9N/b0lwRFZGNzl4OC9t/YXhyZXNkZWZhdWx0/LmpwZw\",\n",
      "    \"engine\": \"brave\",\n",
      "    \"parsed_url\": [\n",
      "      \"https\",\n",
      "      \"www.youtube.com\",\n",
      "      \"/watch\",\n",
      "      \"\",\n",
      "      \"v=MoIpDVF79x8\",\n",
      "      \"\"\n",
      "    ],\n",
      "    \"img_src\": \"\",\n",
      "    \"priority\": \"\",\n",
      "    \"engines\": [\n",
      "      \"brave\"\n",
      "    ],\n",
      "    \"positions\": [\n",
      "      15\n",
      "    ],\n",
      "    \"score\": 0.06666666666666667,\n",
      "    \"category\": \"general\",\n",
      "    \"iframe_src\": \"https://www.youtube-nocookie.com/embed/MoIpDVF79x8\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = \"http://localhost:8001\"  # change if different\n",
    "params = {\n",
    "    \"q\": \"latest AI safety research\",\n",
    "    \"format\": \"json\",\n",
    "    \"categories\": \"general\",   # optional (must match your instance config)\n",
    "    \"pageno\": 1,\n",
    "    \"safesearch\": 1,           # 0,1,2\n",
    "    \"time_range\": \"month\",     # day, month, year (if engines support it)\n",
    "}\n",
    "r = requests.get(f\"{base_url}/search\", params=params, timeout=20)\n",
    "r.raise_for_status()\n",
    "data = r.json()\n",
    "print(data.keys())\n",
    "print(\"Top result:\", json.dumps(data[\"results\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1728a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the publication of the 2025 International AI Safety Report, the number of companies publishing Frontier AI Safety Frameworks has more than doubled, and researchers have refined techniques for training safer models and detecting AI-generated ...\n",
      "\n",
      "We pursue conceptual research that examines AI safety from a multidisciplinary perspective, incorporating insights from safety engineering, complex systems, international relations, philosophy, and other fields.\n",
      "\n",
      "Working closely with the Anthropic Policy and Safeguards teams, Societal Impacts is a technical research team that explores how AI is used in the real world. The Frontier Red Team analyzes the implications of frontier AI models for cybersecurity, biosecurity, and autonomous systems. ... In June, we revealed that we’d set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper.\n",
      "\n",
      "We are currently validating the effectiveness, understandability, and actionability of error pockets through user studies with real AI practitioners who have various model comparison needs. ... Deep reinforcement learning (DRL) is a powerful machine learning paradigm for generating agents that control autonomous systems. However, the black-box nature of DRL agents limits their deployment in real-world safety-critical applications.\n",
      "\n",
      "... MALT (Manually-reviewed Agentic ... (like generalized reward hacking or sandbagging). ... Research on how AI agents can hide secondary task-solving from monitors, finding that harder tasks are more detectable and small models ...\n",
      "\n",
      "The Frontier Safety Framework ... This industry-leading set of protocols developed by Google DeepMind proactively identifies future AI capabilities that could ...\n",
      "\n",
      "Special Issue Description The accelerating pace of recent advances in artificial intelligence highlights the importance of safety in developing and deploying ...\n",
      "\n",
      "As the development and deployment of artificial intelligence (AI) proceed at breakneck speed, increasing attention is being paid to AI safety, an academic discipline which incorporates speculative theoretical work on the alignment of advanced artificial systems with human interests,111See, for example, Bostrom (2014), Russell (2019), and Ngo et al. (2023). technical research on topics like adversarial robustness,222Adversarial robustness studies, and aims at improving, the resilience of AI models against perturbations intended to yield unwanted outputs.\n",
      "\n",
      "In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety. In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit ...\n",
      "\n",
      "It proposes a three-part strategy for states to navigate these challenges: deterrence through Mutual Assured AI Malfunction (MAIM), nonproliferation to prevent rogue actors from acquiring weaponizable AI capabilities, and enhancing competitiveness by bolstering a state's economy and military through AI. Dan Hendrycks, Eric Schmidt, Alexandr Wang ... This paper provides a roadmap for ML Safety and refines the technical problems that the field needs to address. It presents four problems ready for research, namely withstanding hazards (“Robustness”), identifying hazards (“Monitoring”), steering ML systems (“Alignment”), and reducing deployment hazards (“Systemic Safety”).\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utilities import SearxSearchWrapper\n",
    "\n",
    "\n",
    "wrapper = SearxSearchWrapper(searx_host=\"http://localhost:8001\")\n",
    "\n",
    "print(wrapper.run(\"latest AI safety research\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63bb4f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'snippet': 'Since the publication of the 2025 International AI Safety Report, the number of companies publishing Frontier AI Safety Frameworks has more than doubled, and researchers have refined techniques for training safer models and detecting AI-generated ...',\n",
       "  'title': 'International AI Safety Report',\n",
       "  'link': 'https://internationalaisafetyreport.org/',\n",
       "  'engines': ['startpage', 'brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': 'We pursue conceptual research that examines AI safety from a multidisciplinary perspective, incorporating insights from safety engineering, complex systems, international relations, philosophy, and other fields.',\n",
       "  'title': 'Center for AI Safety (CAIS)',\n",
       "  'link': 'https://safe.ai/',\n",
       "  'engines': ['startpage', 'brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': 'Working closely with the Anthropic Policy and Safeguards teams, Societal Impacts is a technical research team that explores how AI is used in the real world. The Frontier Red Team analyzes the implications of frontier AI models for cybersecurity, biosecurity, and autonomous systems. ... In June, we revealed that we’d set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper.',\n",
       "  'title': 'Research - Anthropic',\n",
       "  'link': 'https://www.anthropic.com/research',\n",
       "  'engines': ['startpage', 'brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': 'We are currently validating the effectiveness, understandability, and actionability of error pockets through user studies with real AI practitioners who have various model comparison needs. ... Deep reinforcement learning (DRL) is a powerful machine learning paradigm for generating agents that control autonomous systems. However, the black-box nature of DRL agents limits their deployment in real-world safety-critical applications.',\n",
       "  'title': 'Stanford AI Safety',\n",
       "  'link': 'https://aisafety.stanford.edu/',\n",
       "  'engines': ['startpage', 'brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': '... MALT (Manually-reviewed Agentic ... (like generalized reward hacking or sandbagging). ... Research on how AI agents can hide secondary task-solving from monitors, finding that harder tasks are more detectable and small models ...',\n",
       "  'title': 'METR',\n",
       "  'link': 'https://metr.org/',\n",
       "  'engines': ['startpage', 'brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': 'The Frontier Safety Framework ... This industry-leading set of protocols developed by Google DeepMind proactively identifies future AI capabilities that could\\xa0...',\n",
       "  'title': 'Advancing AI safely and responsibly - Google AI',\n",
       "  'link': 'https://ai.google/safety/',\n",
       "  'engines': ['startpage'],\n",
       "  'category': 'general'},\n",
       " {'snippet': 'Special Issue Description The accelerating pace of recent advances in artificial intelligence highlights the importance of safety in developing and deploying ...',\n",
       "  'title': 'AI Safety | SpringerLink',\n",
       "  'link': 'https://link.springer.com/collections/cadgidecih',\n",
       "  'engines': ['brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': 'As the development and deployment of artificial intelligence (AI) proceed at breakneck speed, increasing attention is being paid to AI safety, an academic discipline which incorporates speculative theoretical work on the alignment of advanced artificial systems with human interests,111See, for example, Bostrom (2014), Russell (2019), and Ngo et al. (2023). technical research on topics like adversarial robustness,222Adversarial robustness studies, and aims at improving, the resilience of AI models against perturbations intended to yield unwanted outputs.',\n",
       "  'title': 'What Is AI Safety? What Do We Want It to Be?',\n",
       "  'link': 'https://arxiv.org/html/2505.02313v1',\n",
       "  'engines': ['brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': 'In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety. In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit ...',\n",
       "  'title': 'AI safety - Wikipedia',\n",
       "  'link': 'https://en.wikipedia.org/wiki/AI_safety',\n",
       "  'engines': ['brave'],\n",
       "  'category': 'general'},\n",
       " {'snippet': \"It proposes a three-part strategy for states to navigate these challenges: deterrence through Mutual Assured AI Malfunction (MAIM), nonproliferation to prevent rogue actors from acquiring weaponizable AI capabilities, and enhancing competitiveness by bolstering a state's economy and military through AI. Dan Hendrycks, Eric Schmidt, Alexandr Wang ... This paper provides a roadmap for ML Safety and refines the technical problems that the field needs to address. It presents four problems ready for research, namely withstanding hazards (“Robustness”), identifying hazards (“Monitoring”), steering ML systems (“Alignment”), and reducing deployment hazards (“Systemic Safety”).\",\n",
       "  'title': 'Research Projects - Center for AI Safety (CAIS)',\n",
       "  'link': 'https://safe.ai/work/research',\n",
       "  'engines': ['startpage', 'brave'],\n",
       "  'category': 'general'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper.results(\"latest AI safety research\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2290e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
